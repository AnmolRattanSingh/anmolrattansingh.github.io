<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Olin College: SP2022 Social Technology Enterprise With Purpose (STEP) on Anmol Rattan Singh Sandhu</title>
    <link>http://localhost:1313/tags/olin-college-sp2022-social-technology-enterprise-with-purpose-step/</link>
    <description>Recent content in Olin College: SP2022 Social Technology Enterprise With Purpose (STEP) on Anmol Rattan Singh Sandhu</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 May 2022 14:53:05 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/olin-college-sp2022-social-technology-enterprise-with-purpose-step/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>IMU Gesture Recognition</title>
      <link>http://localhost:1313/projects/imu-gesture-recognition/</link>
      <pubDate>Sun, 15 May 2022 14:53:05 -0400</pubDate>
      <guid>http://localhost:1313/projects/imu-gesture-recognition/</guid>
      <description>&lt;p&gt;This project analyzed and prepared a baseline machine learning model to perform gesture recognition on data collected with &lt;a href=&#34;https://mbientlab.com/store/metamotionrl/&#34;&gt;MbientLab MMRL IMU&lt;/a&gt; rubber-banded to a two_finger ring. The final baseline model was trained with data from sessions 4, 5, and 7 consisting of 1,212 total instances of 4 gestures collected across 27 people.&lt;/p&gt;&#xA;&lt;p&gt;When trained with a train-test split of 80:20, the model had an accuracy of 75%. The final model trained with full data (no train-test split) had reasonably robust performance in the real-time system (successfully generalized its gestures predictions to other people when integrated with the software demo app).&lt;/p&gt;&#xA;&lt;p&gt;This report provides details on deciding on a gesture set, building and refining the gesture data collection process, and steps to integrate the model with the software iOS demo app.&lt;/p&gt;&#xA;&lt;div class=&#34;center&#34;&gt;&#xA;  &lt;ul class=&#34;actions&#34;&gt;&#xA;       &#xA;    &lt;li&gt;&#xA;      &lt;button&#xA;        class=&#34;button icon outline brands fa-github&#34;&#xA;        onclick=&#34;location.href=&#39;https:\/\/github.com\/OlinSTEP\/signal-processing-gesture-data-collection&#39;&#34;&#xA;        type=&#34;button&#34;&#xA;      &gt;&#xA;        Visit GitHub&#xA;      &lt;/button&gt;&#xA;    &lt;/li&gt;&#xA;    &#xA;  &lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&lt;div id=&#34;Container&#34;&#xA; style=&#34;padding-bottom:56.25%; position:relative; display:block; width: 100%&#34;&gt;&#xA; &lt;iframe id=&#34;googleSlideIframe&#34;&#xA;  width=&#34;100%&#34; height=&#34;100%&#34;&#xA;  src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vT1MJqD9l2cEu7DgdtxDkJb_aH5ysP1NklfoNUuvb3sVjp9z3MezS0HFBOv-fLhV7ESwZks_xU1Z-wQ/embed?start=false&amp;amp;loop=true&amp;amp;delayms=3000&#34;&#xA;  frameborder=&#34;0&#34; allowfullscreen=&#34;&#34;&#xA;  style=&#34;position:absolute; top:0; left: 0&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Reflecting on 400 Hours of Data Collection R&amp;D</title>
      <link>http://localhost:1313/projects/reflecting-on-400-hours-of-data-collection-r-and-d/</link>
      <pubDate>Wed, 11 May 2022 21:37:12 -0400</pubDate>
      <guid>http://localhost:1313/projects/reflecting-on-400-hours-of-data-collection-r-and-d/</guid>
      <description>&lt;p&gt;STEP (Social Technology Enterprise with Purpose) was an 400+ hour experimental course that aimed to give students real-world engineering experience within the freedoms afforded by an education-first structure. I worked on the gesture ring, designing a gesture set and building the data collection process for training a machine-learning gesture classification model.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;30-second demo video&lt;/strong&gt;:&#xA;&#xA;&#xA;    &#xA;    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/gtr_1zKshnQ?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;STEP Gesture Ring Demo&#34;&#xA;      &gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;This 20-page reflection includes criticisms and creative proposals that may be interesting to faculty developing iterations of STEP-like courses,  engineering students grappling with the process of designing a process, and my future self before I set on my next AR/VR, accessibility, machine-learning, or large integrated software development project.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
